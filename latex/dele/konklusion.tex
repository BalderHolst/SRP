\chapter{Konklusion}
\label{ch:Konklusion}

Sorteringsalgoritmers reelle køretider er svære at forudsige, selvom deres opgave lader til at være simpel. De sorterer data på baggrund af dataens attributter. For at klassificere algoritmernes opførsel inddeler vi dem i kompleksitsklasser ved hjælp af store-O-analyse og værste-tilfælde-vækstrate. Binære træer kan bruges til at finde en generel nedre grænse for en sorteringsalgoritmes værste-tilfælde-vækstrate. Denne grænse er en værste-tilfælde-grænse, derfor har insertionsorts bedste-tilfælde-tidskompleksitet på $\Theta (n)$ ingen betydning. Sorteringsalgoritmerne insertionsort og mergesort sorterer begge lister, men er gode til forskellige opgaver: mergesort er god til store input, hvilket forklares af dens tidskompleksitet på $\Theta (n \cdot \log n)$. Det viste sig i testen, at insertionsort i gennemsnit var hurtigere, på computeren algoritmerne blev testet på, hvis $n < 37$. At insertionsort var langt værre til sortering af store lister, var ikke en overraskelse, da vi i afsnit \ref{sec:Analyse af Insertionsort} analyserede algoritmen og kom frem til, at algoritmen havde tidskompleksiteten $O(n^2)$. Det er en meget højere værste-tilfælde-vækstrate end mergesorts. På baggrund af denne analyse kunne de to algoritmer sammensættes til en hybridalgoritme med det formål at optimere algoritmens udførelsestid. Hybridalgoritmen så ud til at være en optimeret version af mergesort, men vi ved ikke tilstrækkeligt om algoritmens tidskompleksitet til at kunne bestemme, om den altid vil være et bedre valg end mergesort.
