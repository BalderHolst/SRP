\chapter{Konklusion}
\label{ch:Konklusion}

Sorteringsalgoritmers reelle køretider er svære at regne med, selvom deres opgave lader til at være simpel. De sorterer data på baggrund af dataens attributter. Derfor inddeler vi dem i kompleksitsklasser ved hjælp af store-O-analyse og værste-tilfælde-vækstrate. Binære træer kunne bruges til at finde en generel nedre grænse for en sorteringsalgoritmes værste-tilfælde-vækstrate. Da denne grænse var en værste-tilfælde-grænse, havde det at insertionsort i bedste tilfælde har tidskompleksiteten $\Theta (n)$ ingen betydning. Sorteringsalgoritmerne insertionsort og mergesort, sorterer begge lister, men er gode til forskellige opgaver: mergesort er god til store input, hvilket forklares af dens tidskompleksitet der var $\Theta (n)$. Det viste sig i testen, at insertionsort i gennemsnit er hurtigere hvis $n < 37$ på computeren algoritmerne blev testet på. At insertionsort var langt værre til sortering af store lister, var ikke en overraskelse, da vi i afsnit \ref{sec:Analyse af Insertionsort}, analyserede algoritmen, og kom frem til at algoritmen havde tidskompleksiteten $O(n)$, hvilket er en meget højere værste-tilfælde-vækstrate end mergesorts. Hybridalgoritmen så ud til at være en optimisering af mergesort, men vi ved ikke tilstrækkeligt nok om algoritmens tidskompleksitet, til at kunne bestemme, om den altid er et bedre valg end mergesort, som dataen eller ser ud til at tyde på.
