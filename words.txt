Alt sorteres. Navne, bøger, lande, slik, biler, ALT. Hver gang vi søger på Google, sorteres søgeresultaterne
efter relevans, og når vi scroler på Tiktok eller Facebook, er opslagene sorteret, på baggrund af tidspunktet
de er postet, og den personlige data der står i deres databaser.
Sortering er en måde at ordne data, så de er nemme at finde igen. Servere verden rundt bruger store dele
af deres ydeevne på netop, at sortere den data de lagre. Optimering af disse algoritmer er derfor essentiel
for det moderne samfunds gigantiske dataafhængighed, da de drastisk kan nedsætte databehandlings ud-
førelsestid.
Denne opgave omhandler sorteringsalgoritmer, og analysen af dem ved hjælp af store-O og kompleksit-
sklasser. Først defineres store-O, og herefter tages der udgangspunkt i sorteringsalgoritmerne insertionsort
og mergesort, hvis tidskompleksitet diskuteres og bevises. Blandt andet vises det at insertionsort har en
værste-tilfælde-vækstrate på O(n2 ). Binære træer bruges i afsnittet efter, til at bevise at den nedregrænse for
værste-tilfælde-vækstraten er O(n · log n). Det vil også forklares hvorfor det, at insertionsort har en bedste-
tilfælde-vækstrate på O(n), ikke overskrider denne grænse. Til sidst implementeres algoritmerne i Python
og deres reelle køretider undersøges. Algoritmerne samles til en hybridalgoritme, er sammenlignes med
mergesort.

Algoritmers Udførelsestid
1.1
Tid som Funktion af Inputtets Størrelse
Dette afsnit bygger primært på side 24 og 25 i bogen "Algoritmer og Datastrukturer" [2]. I følgende afsnit
redegøres for definitionen af store-O
Når vi analyserer algoritmer, er det primære formål at skabe udsagn, der kort og præcist beskriver algorit-
mers opførsel. I vores tilfælde er vi interesserede i algoritmens udførselsestid. Vi lader In være mængden af
mulige argumenter til algoritmen, n være kardinaliteten (størrelsen) af In og I være en instans af I (altså
I ∈ In ). Algoritmens udførselsestid T kan nu beskrives som funktion af instansen T (I). Med det menes der,
at en algoritmes udførselsestid er afhængig af det input algoritmen får. Vi kan bruge dette til at beskrive tre
egenskaber af algoritmens udførselsestid; algoritmens maksimale udførselsestid, algoritmens minimale ud-
førselsestid og algoritmens gennemsnitlige udførselsestid. Den maksimale udførselsestid er udførselsestiden
i værste tilfælde (T (Iværst )), den minimale udførselsestid kan modsat beskrives som køretiden på det bedste
tilfælde (T (Ibedst )), og den gennemsnitlige udførselsestid kan findes ved at tage gennemsnittet af alle ud-
P
1
førselsestiderne ( |I|
I∈In T (I)). Med disse udtryk kan vi nu opskrive algoritmens mulige udførselsestider
som funktion af kardinaliteten n af In således:

T (n) =



 T (Iværst ) : Iværst ∈ Inværste tilfælde


i gennemsnit
T (Ibedst ) : Ibedst ∈ In
P
1
I∈In T (I)
|In |
bedste tilfælde
Alle størrelserne beskriver et aspekt af algoritmens udførselsestid, dog er den mest pålidelige størrelse ud-
førselsestiden i værste tilfælde. Det er samme koncept som når postvæsnet siger at det vil tage tre dage for
din pakke at nå frem, selvom det godt kan ske hurtigere. Det værste tilfælde garanterer hvornår algoritmen
er færdig. Dette værste tilfælde kan sammen med det bedste tilfælde bestemme variansen på udførselsesti-
den. Hvis en algoritme viser stor varians i dens udførselsestid, kan gennemsnitsudførselsestiden sige noget
om hvordan man kan forvente at algoritmen opfører sig.
Denne form for algoritmeanalyse er dog ikke helt problemfri: for det første kan vi aldrig definere nogen fast
forskrift for T (n), der ville gøre det muligt at beregne den præcise udførselsestid. Tildels kan det forklares
af Allan Turings "halt problem"[4], hvor han beviste, at man aldrig kan vide om et program stopper eller
ender i en uendelig løkke. Problemet i vores kontekst, hvor vi forsøger at bestemme en køretid for algo-
ritmen, er, at hvis man kender udførselsestiden for en algoritme, så ville man vide at algoritmen stoppede
før den havde kørt. Det var netop dette, Allan Turing viste, at man ikke kan. En anden forklaring er, at alle


computere vil køre den samme algoritme med forskellig hastighed pga. varians i computerens komponenter.
Derudover vil selv den samme computer aldrig udføre en handling i præcis samme tidsinterval, da andre
processer på computeren kan bruge dele af computerens ydeevne, eller fordi cpu’ens temperatur ændrer
sig. Der er mange faktorer der kan bestemme hvor lang tid det tager en algoritme at fuldføre, hvilket gør
det meget svært at bruge denne form for analyse praktisk.
Uanset hvilken forklaring man bruger, er det svært at sige meget om den faktiske køretid for en algoritme
før man har kørt algoritmen. Derfor er det nemmere og mere brugbart, at forfolde sig til udførselsestidens
vækstrate, og algoritmens opførsel når n bliver meget stort. Denne form for analyse hedder store-O-analyse.
1.2
Store-O-Analyse
I store-O-analyse forholder vi os til hvordan udførselsestidens stiger, når n bliver meget stort. I denne form
for analyse inddeler vi grupper efter deres vækstrate. En funktions vækstrate beskriver hvor hurtigt funk-
tionen stiger i forhold til andre funktioner ved meget høje n-værdier. Hvis en funktion f (n) har en højere
vækstrate end funktionen g(n), betyder det at funktionen f (n) altid vil være større end g(n) ved store nok
n-værdier. Det har ingen betydning om f (n) er mindre end g(n) ved små n, da f (n) altid vil overstige g(n)
pga. dens højere vækstrate. Matematisk kan man sige at en funktion f (n) har en højere vækstrate end g(n),
hvis f (n) ≥ c · g(n) (hvor c er en konstant, og n er tilstrækkeligt stort). Hvis funktionerne har samme væk-
strate gælder det at c ≤
f (n)
g(n)
≤ d (hvor d er en konstant, og det samme gælder for n og c som før). I praksis
betyder det, at funktioner som n2 og n2 + 3n − 10 har den samme vækstrate og, at deres vækstrate er højere
end en funktion som n · log(n).

Vi kan nu bruge disse regler til at definere store-O-notation (se figur 1.1). Lad os begynde med O(f (n)). En
funktion er del af mængden O(f (n)), hvis den kan sættes ind som g(n). Med den matematiske definition
menes der: g(n) skal være sådan at der findes en værdi c, som er større end 0, sådan at der findes et positivt
helt tal n0 , sådan at der for alle n større end n0 , gælder at g(n) er mindre eller lig c · f (n). Dvs. g(n) har
mindre eller samme vækstrate som f (n). I praksis betyder dette, at der altid vil være en positiv n0 , hvorefter
f (n) altid vil være højere end eller lig g(n) (se figur 1.3). Man kan også tænke mængden O(f (n)), som
mængden af funktioner der ikke vokser hurtigere end f (n).
Når man siger at en algoritme er O(n2 ), menes der at vækstraten for T (n) er en del af mængden O(n2 ), og
at T (n) i værste tilfælde har samme vækstrate som n2 . Man kan også sige at algoritmens tidskompleksitet er
O(n2 ) [10]. Da det kun er vækstrate vi interesserer os for, betyder det, at det kun er leddet med den højeste
vækstrate, der betyder noget, fordi de små led vil være ubetydelige ved store n-værdier. En funktion som
n! + n3 + log n vil blive reduceret til tidskompleksiteten O(n!), da n! er leddet med den højeste vækstrate.

Vi kan nu bruge disse regler til at definere store-O-notation (se figur 1.1). Lad os begynde med O(f (n)). En
funktion er del af mængden O(f (n)), hvis den kan sættes ind som g(n). Med den matematiske definition
menes der: g(n) skal være sådan at der findes en værdi c, som er større end 0, sådan at der findes et positivt
helt tal n0 , sådan at der for alle n større end n0 , gælder at g(n) er mindre eller lig c · f (n). Dvs. g(n) har
mindre eller samme vækstrate som f (n). I praksis betyder dette, at der altid vil være en positiv n0 , hvorefter
f (n) altid vil være højere end eller lig g(n) (se figur 1.3). Man kan også tænke mængden O(f (n)), som
mængden af funktioner der ikke vokser hurtigere end f (n).
Når man siger at en algoritme er O(n2 ), menes der at vækstraten for T (n) er en del af mængden O(n2 ), og
at T (n) i værste tilfælde har samme vækstrate som n2 . Man kan også sige at algoritmens tidskompleksitet er
O(n2 ) [10]. Da det kun er vækstrate vi interesserer os for, betyder det, at det kun er leddet med den højeste
vækstrate, der betyder noget, fordi de små led vil være ubetydelige ved store n-værdier. En funktion som
n! + n3 + log n vil blive reduceret til tidskompleksiteten O(n!), da n! er leddet med den højeste vækstrate.

Kompleksitetsklasser
Med denne definition af store-O kan vi klassificere algoritmer i forhold til deres værste-tilfælde-vækstrate.
Store-O inddeler algoritmer i kompleksitetsklasser efter deres tidskompleksitet [3]. Algoritmer som er en
del af klassen O(n2 ) har i værste tilfælde samme vækstrate som n2 . Det kunne også være at algoritmen var
O(n), altså ville den have en lineær vækstrate, og hvis en algoritme har konstant vækstrate, er den del af
kompleksitetsklassen O(1). Det er vigtigt at huske at disse kompleksitetsklasser ikke kun betegner vækstrat-
en i værste, men at de også indeholder alle bedre vækstrater. Det vil altså sige at alle kompleksitetsklasser
med en bestemt værste-vækstrate, også indeholder alle kompleksitetsklasser med lavere værste-vækstrate(se
figur 1.2).

Sorteringsalgoritmer
2.1
Hvad er sortering?
Sortering er helt lavpraktisk, at sætte en mængde data i rækkefølge på baggrund af dataens attributter [9].
Det kunne f.eks. være alfabetisk eller efter farve eller størrelse. Når man som menneske sorterer en hånd
med kort er det ikke altid, at vi tænker over hvordan vi gør. Det er dog ikke altid entydigt hvilken sorterings-
strategi, der ville være hurtigst. Disse sorterings-strategier kan også kaldes sorterings-algoritmer, og det er
ikke kun mennesker, der kan benytte sorterings-algoritmer til f.eks. at sortere kort. Computere kan også, og
de gør det for det meste også langt hurtigere. Resten af denne opgave omhandler to sorterings-algoritmer:
insertionsort og mergesort, og deres forskellige måder at sortere en liste med tal.
Herfra vil jeg kun forholde mig til sortering af lister med tal, og sortere dem på baggrund af deres størrelse
(se figur 2.1). Derudover vil algoritmernes opgave altid være, at sortere liste i stigende rækkefølge.

Insertionsort
Insertionsort er en af de mere simple sorteringsalgoritmer. Pseudokoden til algoritmen kan ses i figur 2.4 på
side 8. Koden er baseret på kilde Algoritmer og datastrukturer [2] s. 104.
Det er ikke en tilfældighed, at denne algoritme hedder insertionsort. Den fungerer nemlig ved at gennemgå
hvert element i listen, og placere det hvor det passer ind i de elementer der allerede er sorterede. Dette er
nok den måde man ville sortere sin hånd i Uno eller 500.
2.2.1
Insertionsort Procedure
Vi kan allerede i linje 2, se at algoritmen gennemgår alle elementerne i listen kronologisk, da algoritmen
her begynder med en for-løkke, der tæller for hvert element i listen, dog starter den ved 2. element i listen.
For at gøre koden mere læsbar, sættes elementet som algoritmen er nået til ind i variablen e i linje 3. Det
næste algoritmen gør, er at checke om elementet har en mindre værdi end det første element i listen, hvis
dette er sandt rykkes alt før elementet et tak til højre, og elementet sættes ind først i listen (se figur 2.2).

Det er vigtigt at pointere, at computere ikke kan "rykke" tal i en liste, men kun overskrive dem. Det er derfor
at det ligner at, computeren har dubleret 5-tallet i figur 2.2. Det har den nemlig, den har taget de tre første
elementer i listen, og skrevet dem over på andet, tredje og fjerde element. Computeren kan nu overskrive
det første tal i listen, da det er en dublet.
Vi er nu nået til linje 11. Hvis ikke elementet er mindre end det første element i listen sker følgende (se
figur 2.3): Algoritmen ser på tallet før elementet og spørger: "er dette tal større end elementet". Hvis det
er, rykkes det til højre. Herefter stiller algoritmen samme spørgsmål, til tallet to pladser før elementet. Hvis
dette tal er mindre and elementet placerer algoritmen elementet efter dette tal i listen, hvis ikke stiller den
samme spørgsmål til tallet 3 pladser før elementet og så videre. Det er vigtigt at pointere, at der altid vil
være et tal der er mindre end elementet, da elementet ellers ville være blevet placeret først i listen af den
første del af proceduren (linje 5-10).

Egenskaber af Insertionsort
Insertionsort er en god sorteringsalgoritme, idet at proceduren er forholdsvis nem at forstå, men også fordi
listen til venstre for det element algoritmen er nået til (e), altid vil være sorteret. Hvis man stopper en
insertionsort-algoritme halvvejs gennem dens køretid, vil man altid have en liste hvor den første halvdel er
sorteret og den anden halvdel usorteret. Denne egenskab har mergesort ikke.
Se afsnit 2.4 for store-O analysen af insertionsort.

Mergesort
Hvor insertionsort er en løkke-baseret algoritme, er mergesort rekursiv idet at den ikke benytter en løkke
til at gentage instruktioner, men kalder sig selv i stedet. Dette leder til en fraktallignende sorteringsmetode,
der deler problemet i mindre og mindre bidder.
2.3.1
Mergesort Procedure
Pseudokoden til denne algoritme kan findes i figur 2.6 på side 10. Koden er basseret på kilde [2, s. 106].
Mergesortalgoritmen består af to funktioner. Lad os begynde med funktionen merge, da det er den mest
simple. Koden til funktionen kan findes på figur 2.6 linje 11-29. Funktionen tager to lister (a og b) som
argumenter, og fletter dem sammen til en tredje liste (c) (se figur 2.5). Denne c liste er dog ikke nødvendigvis
sorteret. Eksempelvis kunne dette være en returneret liste:
merge([1,2,4,8],[9,5,6,4,7]) −→ [1, 2, 4, 8, 9, 5, 6, 4, 7]
Her er resultatet en delvist sorteret liste bestående af flere sorterede dele. Altså kan vi ikke nøjes med merge
for at sortere en liste. En vigtig egenskab af merge er dog at den kan flette to sorterede lister sammen til én
samlet sorteret liste. Det kunne for eksempel være i dette tilfælde:
merge([1,3,4,8],[2,3,6,7,7]) −→ [1, 2, 3, 3, 4, 6, 7, 7, 8]
Her er begge argumenter (a og b) sorterede lister, der flettes sammen til én samlet sorteret liste.
Linjenumre i det næste afsnit refererer til pseudokoden til algoritmen på figur 2.6 på side 10.

Mergesorts kompleksitet kommer dog først rigtigt til udtryk, når vi tager hele algoritmen i betragtning.
Mergesortfunktionen (l. 1) tager en enkel liste som argument, og checker først om listen kun indeholet ét
element (l. 2). Hvis den gør, så returnerer den listen uændret (l. 3). Dette giver intuitivt mening, da en
liste med et enkelt element altid vil være sorteret. Hvis listen er mere end 1 lang, gør den det, som får hele
algoritmen til at fungere (l. 5-8): den deler listen i to, og kalder sig selv på hver halvdel. Herefter sættes de
to dele sammen igen af merge-funktionen. Det er dog ikke helt så enkelt som det lyder, idet at mergesort
rekursivt kalder sig selv. Det leder til at algoritmen splitter listen op igen og igen, indtil der kun er et element
i de mange lister. Herefter samler merge alle de små lister til en stor liste, hvor merge løbende sørger for
at de lister den samler er sorterede (se figur 2.7 på side 10). De lister der er samlet af merge, vil altid være
sorterede, da udgangspunktet er lister med et enkelt element. Disse enkelt-element-lister samles af merge
til lister med to elementer der er sorterede. Og disse sorterede lister sættes sammen med andre sorterede
lister. merge får altid to sorterede lister som input, og spytter derfor altid en samlet sorteret liste ud som
output.
[bro?]
2.3.2
Tidskompleksiteten af Mergesort
Mergesort har tidskompleksiteten O(n · log n) [5]. Algoritmens værste-tilfælde-vækstrate stiger med det
samme som n · log n.

Store-O-Analyse af Insertionsort
I dette afsnit gøres der brug af tidligere definerede begreber og koncepter fra afsnit 1.
2.4.1
Insertionsort i Værste Tilfælde
Når man analyserer en algoritme teoretisk handler det ikke om den faktiske udførelsestid, men nærmere
hvor mange operationer computeren skal køre, for at fuldføre algoritmen. På den måde kan man abstrahere
helt væk fra fysisk tid, og regne med teoretiske operationer, hvis reelle udførelsestid ikke har nogen ind-
flydelse. Det er fordi udførelsestiden er proportional med antallet af operationer der kører på computerens
cpu. Den eneste faktor for antallet af operationer som en sorteringsalgoritme kører, er kardinaliteten af in-
puttet algoritmen udføres på. Dette gør det muligt at beskrive antallet af operationer (der er propportionalt
med udførelsestiden) udelukkende som funktion af n (inputtets kardinalitet). [2, s. 42]
Insertionsort består af to løkker: en indre og en ydre. Den ydre løkke kører fra i = 1 til (og uden) i = n.
Altså kører koden inde i løkken n − 1 gange. Den indre løkke kører i værste tilfælde fra j = i til j = 1 for
hver i værdi i den ydre løkke. Det betyder at operationerne, som den indre løkke udfører, stiger sammen
med i (se figur 2.8). Vi ender med at kunne beskrive det totale antal gange som koden i den indre løkke
kører, som summen af alle i-værdierne, som den ydre løkke gennemgår, koden i den indre lykke køres i − 1
gange for hver i.
n
X
i−1
i=2
Den indre løkkes kode køres i gange, da løkken gennemgår fra j = i til og med j = 1, altså kører løkken i
gange. Vi kan nu omskrive summen således:
n−1
X
i
i=1
For at simplificere udtrykket endnu mere kan vi bruge reglen at en sum som 1 + · · · + n − 1 kan omskrives
på denne måde. [Bevis??]
n−1
X
i =
i=1
n(n − 1)
2
=
1 2 1
·n − ·n
2
2
Dette er altså et udtryk for hvor mange gange koden i den indre løkke kører, som funktion af kardinaliteten
af algoritmens input. Vi kunne nu begynde at gange denne funktion med hvor mange operationer der sker
inde i den indre løkke. Og lægge operationerne der sker n gange til, fordi de er med i den ydre løkke. Til
sidst kunne vi lægge de operationer der kun sker én gang til, og resultatet ville se nogenlunde sådan ud:
a·
1 2
1
·n −a· ·n+b·n+c
2
2
Her er a antallet af operationer i den indre løkke, b er antallet af operationer kun i den ydre løkke, og c er
operationerne som er uden for begge løkker. Det smarte ved store-O-notation er at vi faktisk ikke behøver
at kende de reelle værdier for a, b og c, da de alle er konstanter, og derfor ikke har nogen indflydelse
på algoritmens vækstrate. Selv ikke udførelsestiden for hver at disse operationer, har nogen indflydelse på

vækstraten. Vi kan derfor sige at algoritmens udførelsestid er O(n2 ).
Tinsertionsort (n) ∈ O(n2 )
Med det menes der at udførelsestiden i værste tilfælde har tidskompleksiteten Θ(n2 ).

Binære Træer
3.1
Hvad er et Træ
Træer som vi kender dem, har en stamme hvorfra grene springer ud. Fra grenene springer mindre grene,
og på den måde ender træet med at have mange små grene, som stammer fra samme stamme. Infor-
matikkens træer er ikke langt fra denne forståelse. I informatik er træer en datastruktur. Det er et netværk
af knudepunkter forbundet af kanter. Træet starter med roden, der typisk er tegnet øverst (se figur 3.1).
En rod er hvor træet begynder, og har ingen forældre. Fra roden forgrener træet sig til flere, børn der også
kan have flere børn. Træstrukturen er sådan at man altid kan vælge et knudepunkt, og lave et subtræ med
kudepunktet som rod. Knudepunkterne hvor træet ender kalder man blade. Informatikkens træer har også
en dybde. Hvis man starter ved roden, er dybden 0. Hver gang man går et trin "ned", i træstrukturen, stiger
dybden med 1. Det betyder at den maksimale dybde svarer til træets højde. [7]
3.2Antallet af Knudepunter og Kanter
3.3Højden af et Balanceret og Fuldt Træ
3.4Et Binært Træ
Man tegner normalt binære træer som på figur 3.2 bestående af knudepunkter og forbindelser mellem dem.
Det specielle ved binære træer, er at hvert knudepunkt højst må forgrene sig to gange (altså binært). Denne
lille specialisering er brugbar i mange sammenhænge som f.eks. som datastruktur til søgealgoritmer [1],
men det viser sig også at være en snedig vej til at beskrive sorteringsalgoritmers opførsel.

ed første blik kunne man tilgives for ikke, at se hvordan binære træer har relevans for sorteringsalgoritmer,
men det kræver bare, at man giver knudepunkterne og grenene meningsfulde betydninger. I bund og grund
er det som en sorteringsalgoritme gør, at fortage en masse sammenligninger af elementerne i dens input.
Algoritmen bestemmer hvilke operationer, den skal køre udelukkede på baggrund af disse sammenligninger.
Hvis vi tænker hvert knudepunkt som en sammenligning af to elementer fra algoritmens input (se figur 3.3),
er det jo givet, at det ene element enten vil være større end det andet eller ikke større. Denne sammenligning
guider algoritmens operationer og derved den næste sammenligning. Dette gentages til algoritmen har
gjort nok sammenligninger, til at kunne sortere dens input. Vi kan derfor tænke alle træets blade, som en
måde for algoritmen at sortere et bestemt input; Hvert blad har kun en specifik vej, og vejen til bladet er
udelukkede givet af algoritmens input. Altså er hvert af træets blade en kollektion af de sammenligninger,
som algoritmen gjorde for at sortere inputtet. [2, s. 109]

Det Mindste antal Sammenligninger
I dette store teoretiske træ med alle dets blade kunne man jo så spørge: Hvor mange sammenligninger skal
der så højest til at sortere et input? eller omformuleret: Hvor højt er træet? Denne oversættelse holder stik,
da vi skal lave lige så mange sammenligninger for at komme ned til nederste blad, som træet er højt.

Højden af et Binært Træ
Dette bevis er basseret på kilden [2, s. 109].
Bevis: I et sammenligningstræ for en sorteringsalgoritme med input længden n, hvor π og σ er permutation-
er af listen {1 . . . n}, er bladene der svarer til π og σ (lπ og lσ ) forskellige.
Dette er et modstridsbevis hvilket betyder, at vi starter med at antage det modsatte, af det vi prøver at
bevise: vi antager altså at π og σ, leder til samme blad i sammenligningstræet; lπ og lσ er ens.
Under denne antagelse kan to forskellige lister {e1 . . . en } og {e01 . . . e0n }, gennemgå de præcis samme oper-
ationer og derved sortere listen. Dette er en modstrid, da man aldrig vil kunne benytte præcis de samme
operationer, til at sortere to forskellige permutationer af {1 . . . n}. 
Alle permutationer af listen {1 . . . n} har altså et tilsvarende blad, derfor må sammenligningstræet have
mindst n!, da der skal være et for hver permutation.
Antallet af balde på et binært træ er 2d , hvor d er træets dybde. Vi kan nu opstille denne ligning:
2d ≥ n! ⇔ d ≥ log n!

n n
e
For at bygge videre kan vi nu bruge regnereglen n! ≥
d ≥ log n! ≥ log
 n n 
e
Ved hjælp af logaritmeregneregler kan vi omskrive udtrykket længst til højre således:
log
 n n 
e
= n · log
n
e
= n · log n − n · log e
Dette er altså et udtryk for højden af et sammenligningstræ:
d ≥ n · log n − n · log e
Da d er træets dybde, er det også det maksimale antal sammenligninger en given sorteringalgoritme skal
gøre, for at sortere en liste. Udtrykket siger at den nedre grænse for d er n · log n − n · log e. Det er altså ikke
muligt at sortere en liste med færre sammenligninger i værste tilfælde. Da antallet af sammenligninger er
proportionalt med algoritmens udførelsestid, må det være sandt at alle sorteringsalgoritmer har en nedre
værste-tilfælde-vækstrategrænse på O(n · log n).

3.7
Store-O er Værste Tilfælde
Det er en vigtigt detalje, at denne nedre grænse er for vækstraten i værste tilfælde. Insertionsort kan i bedste
tilfælde sortere en liste med O(n) [5], men det er værste tilfælde som vækstraten-gænsen gælder for. Det
er muligt at sortere en liste hurtigere end O(n · log n), det kræver bare at det ikke er det værste tilfælde,
Side 15 af 33Sorø Akademi
Balder W. Holst 3.x
og at algoritmen (som insertionsort) kan sortere listen hurtigere i bedste tilfælde. Man burde hellere tænke
grænsen således: Man kan aldrig lave en sorteringalgoritme, der virker ved hjælp af sammenligninger, som
vi kan være sikrer på, er hurtigere end O(n · log n).

Mergesort og Den Nedre Grænse
Vi ved nu at alle sorteringsalgoritmers værste-tilfælde-vækstrate, er afgrænset af O(n · log n). Vi ved også at
mergesort har tidskompleksiteten O(n · log n). Mergesort har altså den teoretisk bedste tidskompleksitet for
en sammenlignings-sortering-algoritme. Ikke nok med det betyder det også at mergesort både i bedste og
væreste tilfælde er O(n · log n). Mergesort er altså Θ(n · log n).
Tmergesort (n) ∈ Θ(n · log n)

Implementering og Test
I dette afsnit testes algoritmerne fra afsnit 2. Algoritmerne implementeres i python.
4.1
Python
Sproget Python har et højt abstraktionsniveau, og kan på mange punkter ligne vores pseudokode fra tidligere
[2, s. 68]. Det betyder at vi på mange punkter opgiver lidt kontrol til compileren, for at gøre koden mere
læsbar og tilgængelig. Det betyder også at vi aldrig helt kan vide hvilke instruktioner computeren udfører.
Derudover er Python et relativt langsomt sprog, da koden kompileres samtidigt med at programmet kører.
Dette gør at sproget ikke egner sig særligt godt til at skrive optimerede algoritmer, der skal køre på meget
store datasæt. Heldigvis kan vi se bort fra dette, da vi med store-O-analyse er ligeglade med den reelle
udførelsestid, og i stedet er interesseret i vækstrater. Vækstraten for en algoritme er nemlig egns for en
algoritme, ligegyldigt hvor langsomt hvert skridt er. På nogle punkter er det måske endda en fordel, at
algoritmerne køre langsommere, da det vil gøre forskellen i udførelsestiderne større, og lettere at forholde
sig til.
4.2
Implementering
Implementering i Python er relativt simpelt. Python bruger dynamiske variabler [11], derfor er det ikke
nødvendigt at fortælle Python, om en variabel f.eks. er et heltal eller en liste.

Test af Sorteringsalgoritmer
For at teste sorteringsalgoritmerne fra afsnit 2, og se hvordan deres reelle udførelsestid som funktion af n,
relaterer til deres teoretiske væksthastighed og store-O-analyse, skal algoritmerne køres på en kontroleret
måde hvor vi systematisk kan tage tid på algoritmens udførelsestid ved forskellige input.
På figur 4.1 på side 18 ses koden til test af algoritmerne. Variablen f unctions på linje 1 indeholder en liste
med de algoritmer, der skal testes. I vores tilfælde indeholder den [insertionsort,mergesort]. Inde i denne
løkke tester vi funktionerne. Det at vi behandler algoritmerne ens, sikrer at de bliver testet på samme måde.
Det næste vi gør er at definere lister til opbevarting af kardinaliteten af den liste som algoritmen sorterer
(l. 4), og en til den reelle tid det tager for algoritmen at sortere listen (l. 6). I linje 9 sætter vi frøet for de
pseudotilfældige tal [8], som vi senere skal bruge til at generere listerne som sorteringsalgoritmerne skal
sortere. Det er vigtigt af vi sætter frøet til den samme værdi, før vi tester hver algoritme, da vi på den måde

sikrer os, at det er de samme pseudotilfældige lister, som algorimerne sorterer. Vi kan derfor med god vilje
sammenligne algoritmernes køretider, da vi er sikre på at deres input, var det samme under testen. Efter
denne initialisering begynder vi to løkker (l. 12 og 15). Den første sørger for at vi gentager den samme test
flere gange. I vores tilfælde er variablen trials sat til 15, hvilket resulterer i at vi tester sorteringen af hver n
lange liste 15 gange. Den indre løkke tæller fra i = 0 til i = 80. Vi bruger herefter i-værdien til at generere
vores n-værdier i linje 18. Til det bruges formlen n = b1.1i e. Valget at test med exponentielt stigende n, er
af to grunde: For det første kan det tage lang tid for en algoritme, at sortere lister med store n, så det gør
hele processen meget hurtigere, hvis man ikke tester så mange sorteringer med store n. Den anden og nok
bedre grund, er at vi er meget interesserede i at se hvordan algoritmerne måles med hinanden, når n ikke
er stort. Det ville jo være interessant, hvis vi kunne identificere et n0 som i afsnit 1. Til dette skal vi bruge
en punkttæthed, der ved store n ville være overflødig.
Nu er det hele sat op, og vi kan teste algoritmen på en n lang liste med tilfældige tal. n-værdien og den tid
sorteringen tog, gemmes i listerne, og eksporteres til en csv fil til brug i databehandlingen (se koden i bilag
1 og 2).

De Reelle Udførelsestider
Algoritmernes udførelsestider er plottet i figur 4.2 på side 20.
Som det tydeligt fremgår på graferne, er mergesort langt hurtigere end insertionsort ved store n. Vi kan
også se at algoritmerne nogenlunde følger deres regressioner, men hvorfor egentlig det? Det er kun i værste
tilfælde at Tinsertionsort (n) ∈ Θ(n2 ) ikke? Jo, men det lader til at insertionsorts gennemsnitlige vækstrate,
er tættere på Θ(n2 ) end på Θ(n). Det er dog alligevel tydeligt at andengradsregression ikke passer helt på
insertionsorts gennemsnitlige køretid, hvis vi zoomer ind på plottet til højre på figur 4.2. Faktisk passer
regressionen slet ikke på punkterne på dette stykke af grafen, hvilket tyder på at algoritmens udførelses-
tid ikke bare kan beskrives med et andengradspolynomie. Hvad angår mergesort følger den regressionen
rigtig godt. Det tyder på at antagelsen at mergesorts vækstrate altid er Θ(n log n) fra afsnit 3.8 må være sand.
En anden finurlighed kan findes i residualplottet (figur 4.3). Residualplottet for begge grafer er rigtig flot,
med en jævn punktfordeling på begge sider af regressionen, men til aller sidst er der et par af insertionsorts
punkter, der er langt under regressionen. Det kan være af flere grunde blandt andet at computerens styresys-
tem lige i dette tilfælde, tildelte flere ressourcer til programmet. En anden grund kunne være at algoritmen
netop i disse tilfælde, var heldig at få en delvist sorteret liste, der gjorde at listen kunne sorteres langt hur-
tigere end O(n2 ). Dog tyder det tydelige nedadgående mønster i outlier-residualerne på, at afvigelsen ikke
er en tilfældighed.
Når man ser på residualplottene er et andet mønster også tydeligt: variansen stiger. Udførelsestiderne har
større og større usikkerhed, jo større n er. Dette giver intuitivt mening hvis man sætter det i kontekst.
Hvis 10 personer løber et 50m løb, vil alle løbere afslutte inden for et forholdsvist lille tidinterval. Hvis
vi herefter bad dem om at løbe et maraton, ville deres tider ikke bare variere med sekunder og minutter,
men med timer. Variansen i deres tider, øges altså af størrelsen på opgaven. Det samme er sandt for vores
sorteringsalgoritmer, hvilket tydeliggøres af residualplottene.
4.4.1
Hvornår er Insertionsort Hurtigst?
Det er tydeligt at mergesort er langt bedre, til at sortere store lister, men ved små n kan vi også se at inser-
tionsort, ikke er langt bagefter. Faktisk er den gennemsnitlige sortering hurtigere eller nogenlunde ens før
n = 37 (se figur 4.4). Det bekræfter teorien fra afsnit 1, idet at store-O ikke siger noget om algoritmens
opførsel ved små n, men kun ved meget store. Her er det tydeligt at en sådan form for analyse ikke er
tilstrækkeligt for at bedømme, hvilken algoritme der er bedst til hvilke problemer. Store-O analyse er også
blind overfor den gennemsnitlige udførelsestid. Ved små n er den gennemsnitlige køretid brugbar, da den
maksimale køretid aldrig bliver astronomisk stor, og vi kan derfor med god vilje bruge gennemsnitlig ud-
førelsestid i disse overvejelser. Det ville gennemsnitligt være hurtigst, at bruge insertionsort, ved små n og
mergesort ved store. Punktet der adskiller hvilken algoritme der er hurtigst er ca. n = 37. Dette punkt kan
siges at svarer til n0 fra afsnit 1, idet at mergesort gennemsnitligt altid er hurtigere efter punktet. Alligevel
er det ikke helt sandt, da n0 fra afsnit 1 forholder sig til værste-tilfælde-udførelsestiden, men vi kan i denne
reelle kontekst bruge n0 , som punktet hvorefter mergesort gennemsnitligt er hurtigst.

Optimering af Mergesort
At insertionsort faktisk normalt er hurtigere end mergesort hvis n < n0 , betyder at den hurtigste måde
at sortere en liste afhænger af listens længde: Er listen på under n0 elementer? så brug insertionsort. Er
listen over n0 elementer? så er mergesort gennemsnitligt hurtigere. Dette kunne lede os til at lave en ny
sorteringsalgoritme således:

Her bruger vi insertionsort hvis listens længde er under eller lig 37, og mergesort hvis ikke. Det er en
forbedring af algoritmen, men kun hvis n < 37. Dog er der en endnu mere snedig måde at inkorporere denne
ide. Hvis vi tænker tilbage på mergesorts procedure fra afsnit 2, ved vi at algoritmen deler en usorteret liste
op indtil der kun er lister med enkle elementer tilbage. Funktionen merge sætter herefter listerne sammen
igen, og sørger for at den samlede liste altid vil være sorteret, da den altid for sorterede lister som input. Vi
ved dog nu at denne process, ikke er effektiv ved n-værdier under 37. Det er derfor en smart ide at bruge
insertionsort til at sortere listerne når mergesort har opdelt listen i tilstrækkeligt små bidder. Koden for
denne hybridalgoritme kan ses på figur 4.6. I stedet for at opdele listen helt til der kun er et enkelt element
i hver delliste, stopper denne algoritme opdelingen så snart insertionsort kan sortere listen hurtigere. Det er

vigtigt at pointere, at algoritmernes udførelsestid er nogenlunde ens ved n ≈ 37, og at det derfor ikke ville
være hurtigere, at splitte listen op med mergesort, til alle dellisterne havde en længde på 37, og derefter
sortere med insertionsort. For at sikrer at insertionsort kun sorterer lister hvor den er hurtigst, splitter
hybridalgoritmen inputtet, til dellisterne er højest 30 elementer lange.
4.5.1
Sammenligning af Optimerede Algoritmer
Udførelsestiderne er plottet på figur 4.7.
Når man ser på grafen er det klart, at hybridalgoritmen gennemsnitligt er hurtigst, men ikke i samme
grad som mergesort var insertionsort overlegen ved høje n. Det ligner på grafen, at hybridalgoritmen lige-
som mergesort gennemsnitligt kører med Θ(n · log n). Vi kan dog ikke være sikker på hybridalgoritmens
tidskompleksitet, da vi ikke har fortaget den teoretiske analyse af algoritmen. Hvad der er sikkert, er at
hybridalgoritmen er hurtigere end mergesort på computeren som algoritmen er testet på. Det ser dog ud
til at algoritmerne i hvert faldt gennemsnitligt, har nogenlunde samme vækstrate. En anden måde vi kan
argumentere for at hybridalgoritmens tidskompleksitet i hvert fald ikke er mindre end O(n log n), er (som vi
beviste i afsnit 3.6) at enhver sorteringsalgoritme der fungerer ved brug af sammenligninger, ikke kan have
en mindre tidskompleksitet end O(n · log n). Hybridalgoritmen har enten den samme eller værre tidskom-
pleksitet end mergesort. Det der skaber forskellen må være en et mindre led i hybridalgoritmens T (n) eller
en konstant, der ganges på det dominerende led.

Nu ser det ud som om at hybridalgoritmen altid er det bedste valg ved store n, men det kan vi faktisk ikke
være sikre på. Problemet er, at det eneste vi ved om hybridalgoritmens tidskompleksitet, er at algoritmen i
værste fald er Ω(n · log n) ligesom alle andre sammenlignings-sorterings-algoritmer. Det betyder, at vi ikke

ved om hybridalgoritmen ved et exceptionelt dårligt udfald, har en astronomisk lang udførelsestid. Der er
altså ingen garanti for algoritmens udførelsestid. Mergesort på den anden side, ved vi har tidskompleksitet
Ω(n · log n), hvilket garanterer udførelsestidens vækstrate. Selvfølgelig ville dette problem forsvinde, hvis
det viste sig, at hybridalgoritmen også var Θ(n · log n), men da vi ikke ved det med sikkerhed, kan vi
ikke være sikre på, at hybridalgoritmen altid er bedre ved store n-værdier. Hvis det derimod viste sig, at
hybridalgoritmen havde en tidskompleksitet på O(n2 ) ligesom insertionsort (som den jo gør brug af), ville
det betyde at algoritmen netop, ville have en ekstrem lang udførelsestid i værste tilfælde. Så ville den pæne
graf på figur 4.7 være ret så ligegyldig, da et input med høj kardinalitet i værste tilfælde ville tage utrolig
lang tid. Den lille mængde tid hybridalgoritmen i gennemsnit sparer, ville da ikke være risikoen for meget
lange udførelsestider værd.

Konklusion
Sorteringsalgoritmers reelle køretider er svære at regne med, selvom deres opgave lader til at være simpel.
De sorterer data på baggrund af dataens attributter. Derfor inddeler vi dem i kompleksitsklasser ved hjælp af
store-O-analyse og værste-tilfælde-vækstrate. Binære træer kunne bruges til at finde en generel nedre grænse
for en sorteringsalgoritmes værste-tilfælde-vækstrate. Da denne grænse var en værste-tilfælde-grænse, havde
det at insertionsort i bedste tilfælde har tidskompleksiteten Θ(n) ingen betydning. Sorteringsalgoritmerne
insertionsort og mergesort, sorterer begge lister, men er gode til forskellige opgaver: mergesort er god til
store input, hvilket forklares af dens tidskompleksitet der var Θ(n). Det viste sig i testen, at insertionsort
i gennemsnit er hurtigere hvis n < 37 på computeren algoritmerne blev testet på. At insertionsort var
langt værre til sortering af store lister, var ikke en overraskelse, da vi i afsnit 2.4, analyserede algoritmen,
og kom frem til at algoritmen havde tidskompleksiteten O(n), hvilket er en meget højere værste-tilfælde-
vækstrate end mergesorts. Hybridalgoritmen så ud til at være en optimisering af mergesort, men vi ved ikke
tilstrækkeligt nok om algoritmens tidskompleksitet, til at kunne bestemme, om den altid er et bedre valg
end mergesort, som dataen eller ser ud til at tyde på.

Chars:
37446
